---
layout: page
title: CS231N 解题报告
showbar: false
---

*本文不包含任何代码！*

# Assignment 1

## KNN

训练数据$$Train_{m\times p}$$，测试数据$$Test_{n\times p}$$，测试数据和训练数据之间的距离矩阵$$distance_{n\times m}$$

距离矩阵计算公式：$$ distance_{ij}=\sqrt{\sum_{k}^{p}{(Test_{ik}-Train_{jk})^2}} $$

### 1 双循环

循环体内单独计算$$distance_{ij}$$，直接按照公式计算即可

### 2 单循环

只对i循环，整行一起计算，利用numpy的广播$$ distance_{i}=\sqrt{\sum_{k}^{p}{(Test_{ik}-Train_{column\ k})^2}} $$

单循环比双循环慢

### 3 无循环

整个矩阵一起计算，同样利用numpy的广播，把平方项展开：

$$
\begin{align}
distance_{ij}&=\sqrt{\sum_{k}{(Test_{ik}-Train_{jk})^2}}\\
&=\sqrt{\sum_{k}{Test_{ik}^2}+\sum_{k}{Train_{jk}^2}-2\sum_{k}{Test_{ik}*Train_{jk}}}
\end{align}
$$

根号中：第一项与$$j$$无关，对$$Test$$每行求平方和后的列向量广播到$$distance$$的每一列；第二项与$$i$$无关，对$$Train$$的每行求平方和后转置得到行向量广播到$$distance$$的每一行；第三项正好是矩阵乘法。所以有：

$$
\begin{align}
distance&=\sqrt{-2\langle Test,Train^T\rangle+Te_{n\times 1}+Tr_{1\times m}}
\end{align}
$$

其中$$Te_i=\sum_{k}{Test_{ik}^2}$$，$$Tr_j=\sum_{k}{Train_{jk}^2}$$

### 4 Cross-Validation

5个Fold，5个batch，每个batch取4个Fold训练，剩下的一个验证。每个batch需要验证所有的k值。

外循环batch，内循环k，计算每个batch不同k的精度

从图上取最佳的k作为最终值

## SVM

数据预处理：使得训练数据0均值，其余数据集做相同加减

### 线性SVM

对训练数据扩展一维$$TrainEx_{m\times (p+1)}$$，$$TrainEx_{i}=\begin{bmatrix}Train_{i}&1\end{bmatrix}$$，消掉偏移项

$$
\begin{cases}
loss&=reg*{\Vert W \Vert}^2+\frac{1}{n}\sum_i^n\sum_j^mmargin_{ij}\\
grad&=2reg*W+\frac{1}{n}\sum_i^n\sum_j^m{\nabla margin_{ij}}\\
margin_{ij}&=\begin{cases}max(0,\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1)&j\ne y_i\\0&j=y_i\\\end{cases}
\end{cases}
$$

### 1 循环计算

由于整体难以计算，因此单独计算$$margin_{ij}$$及其梯度，然后循环求和。只有非0项才参与$$loss$$和$$grad$$的计算，因此有

$$
\begin{align}
margin_{ij}&=\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1\\
\nabla margin_{ij}&=\nabla \langle X,W\rangle_{ij}-\nabla \langle X,W\rangle_{iy(i)}\\
&=\begin{bmatrix}
0_{00}&\cdots&0_{0(j-1)}&X_{i0}&0_{0(j+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(j-1)}&X_{i1}&0_{1(j+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(j-1)}&X_{i(n-1)}&0_{(n-1)(j+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}\\
&-
\begin{bmatrix}
0_{00}&\cdots&0_{0(y_i-1)}&X_{y_i0}&0_{0(y_i+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(y_i-1)}&X_{y_i1}&0_{1(y_i+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(y_i-1)}&X_{y_i(n-1)}&0_{(n-1)(y_i+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}
\end{align}
$$

### 2 整体计算

利用numpy矩阵计算的优势

因为只有$$margin_{ij}\neq 0$$的情况下才有必要计算，因此可以构造辅助矩阵来记录其是否为0

$$
\begin{align}
dW_{ij}&=\sum_k^n\sum_l^m({\nabla \langle X,W\rangle_{kl}}_ {ij} -{\nabla \langle X,W\rangle_{ky(k)}}_{ij})\\
&=\sum_{k,margin_{kj}\gt 0}^n{X_{ki}}-\sum_{k,y(k)=j}^n\sum_{l,margin_{kl}\gt 0}^p{X_{ki}}
\end{align}
$$

$$
\begin{align}
dW&=\langle X^T,M1\rangle-\langle X^T,M2\rangle\\
M1_{ij}&=
\begin{cases}
1&margin_{ij}>0\\
0&otherwise
\end{cases}\\
M2_{ij}&=
\begin{cases}
\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
\end{align}
$$

记辅助矩阵$$C_{n\times m}=M1-M2$$，有：

$$
C_{ij}=
\begin{cases}
1&margin_{ij}>0\\
-\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
$$

化简后的梯度计算：

$$\sum_i^n\sum_j^m{\nabla margin_{ij}}=\langle X^T,C\rangle$$

### 3 SGD

随机选择batch，计算loss和grad

梯度下降$$W=W-lr*grad$$

### 4 预测

自定义lr和L2的range，两层循环遍历所有组合，记录验证精度最高的svm

## Softmax

