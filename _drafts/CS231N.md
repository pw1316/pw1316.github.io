---
layout: page
title: CS231N 解题报告
showbar: false
---

*本文不包含任何代码！*

*<>为矩阵点积，\*为矩阵逐元素相乘*

# Assignment 1

## KNN

训练数据$$Train_{m\times p}$$，测试数据$$Test_{n\times p}$$，测试数据和训练数据之间的距离矩阵$$distance_{n\times m}$$

距离矩阵计算公式：$$ distance_{ij}=\sqrt{\sum_{k}^{p}{(Test_{ik}-Train_{jk})^2}} $$

### 1 双循环

循环体内单独计算$$distance_{ij}$$，直接按照公式计算即可

### 2 单循环

只对i循环，整行一起计算，利用numpy的广播$$ distance_{i}=\sqrt{\sum_{k}^{p}{(Test_{ik}-Train_{column\ k})^2}} $$

单循环比双循环慢

### 3 无循环

整个矩阵一起计算，同样利用numpy的广播，把平方项展开：

$$
\begin{align}
distance_{ij}&=\sqrt{\sum_{k}{(Test_{ik}-Train_{jk})^2}}\\
&=\sqrt{\sum_{k}{Test_{ik}^2}+\sum_{k}{Train_{jk}^2}-2\sum_{k}{Test_{ik}*Train_{jk}}}
\end{align}
$$

根号中：第一项与$$j$$无关，对$$Test$$每行求平方和后的列向量广播到$$distance$$的每一列；第二项与$$i$$无关，对$$Train$$的每行求平方和后转置得到行向量广播到$$distance$$的每一行；第三项正好是矩阵乘法。所以有：

$$
\begin{align}
distance&=\sqrt{-2\langle Test,Train^T\rangle+Te_{n\times 1}+Tr_{1\times m}}
\end{align}
$$

其中$$Te_i=\sum_{k}{Test_{ik}^2}$$，$$Tr_j=\sum_{k}{Train_{jk}^2}$$

### 4 Cross-Validation

5个Fold，5个batch，每个batch取4个Fold训练，剩下的一个验证。每个batch需要验证所有的k值。

外循环batch，内循环k，计算每个batch不同k的精度

从图上取最佳的k作为最终值

## SVM

数据预处理：使得训练数据0均值，其余数据集做相同加减

### 线性SVM

对训练数据扩展一维$$TrainEx_{m\times (p+1)}$$，$$TrainEx_{i}=\begin{bmatrix}Train_{i}&1\end{bmatrix}$$，消掉偏移项

$$
\begin{cases}
loss&=reg*{\Vert W \Vert}^2+\frac{1}{n}\sum_i^n\sum_j^mmargin_{ij}\\
grad&=2reg*W+\frac{1}{n}\sum_i^n\sum_j^m{\nabla margin_{ij}}\\
margin_{ij}&=\begin{cases}max(0,\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1)&j\ne y_i\\0&j=y_i\\\end{cases}
\end{cases}
$$

### 1 循环计算

由于整体难以计算，因此单独计算$$margin_{ij}$$及其梯度，然后循环求和。只有非0项才参与$$loss$$和$$grad$$的计算，因此有

$$
\begin{align}
margin_{ij}&=\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1\\
\nabla margin_{ij}&=\nabla \langle X,W\rangle_{ij}-\nabla \langle X,W\rangle_{iy(i)}\\
&=\begin{bmatrix}
0_{00}&\cdots&0_{0(j-1)}&X_{i0}&0_{0(j+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(j-1)}&X_{i1}&0_{1(j+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(j-1)}&X_{i(n-1)}&0_{(n-1)(j+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}\\
&-
\begin{bmatrix}
0_{00}&\cdots&0_{0(y_i-1)}&X_{y_i0}&0_{0(y_i+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(y_i-1)}&X_{y_i1}&0_{1(y_i+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(y_i-1)}&X_{y_i(n-1)}&0_{(n-1)(y_i+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}
\end{align}
$$

### 2 整体计算

利用numpy矩阵计算的优势

因为只有$$margin_{ij}\neq 0$$的情况下才有必要计算，因此可以构造辅助矩阵来记录其是否为0

$$
\begin{align}
dW_{ij}&=\sum_k^n\sum_l^m({\nabla \langle X,W\rangle_{kl}}_ {ij} -{\nabla \langle X,W\rangle_{ky(k)}}_{ij})\\
&=\sum_{k,margin_{kj}\gt 0}^n{X_{ki}}-\sum_{k,y(k)=j}^n\sum_{l,margin_{kl}\gt 0}^p{X_{ki}}
\end{align}
$$

$$
\begin{align}
dW&=\langle X^T,M1\rangle-\langle X^T,M2\rangle\\
M1_{ij}&=
\begin{cases}
1&margin_{ij}>0\\
0&otherwise
\end{cases}\\
M2_{ij}&=
\begin{cases}
\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
\end{align}
$$

记辅助矩阵$$C_{n\times m}=M1-M2$$，有：

$$
C_{ij}=
\begin{cases}
1&margin_{ij}>0\\
-\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
$$

化简后的梯度计算：

$$\sum_i^n\sum_j^m{\nabla margin_{ij}}=\langle X^T,C\rangle$$

### 3 SGD

随机选择batch，计算loss和grad

梯度下降$$W=W-lr*grad$$

### 4 预测

自定义lr和L2的range，两层循环遍历所有组合，记录验证精度最高的svm

## Softmax

输入$$X_{n\times d}$$，参数$$W_{d\times c}$$，标签$$y_{n\times 1}$$

首先是线性层：

$$Score=\langle X,W\rangle$$

Softmax值：

$$Softmax_{ij}=\frac{e^{Score_{ij}}}{\sum_k{e^{Score_{ik}}}}$$

损失函数和梯度：

$$
\begin{cases}
loss&=L2-\frac{1}{n}\sum_i{ln(Softmax_{iy(i)})}\\
grad&=\nabla L2+\frac{1}{n}\sum_i\frac{1}{Softmax_{iy(i)}}\nabla Softmax_{iy(i)}
\end{cases}
$$

### 1 循环求解

主要关注$$Softmax$$矩阵，主循环对该矩阵的每一行进行计算

损失直接代公式即可，对于梯度

$$
\begin{align}
\frac{\partial loss}{\partial W_{kl}}&=\sum_{i,j}\frac{\partial loss}{\partial Score_{ij}}\frac{\partial Score_{ij}}{\partial W_{kl}}\\
&=\sum_{i}\frac{\partial loss}{\partial Score_{il}}X_{ik}\\
&=\sum_{i}\frac{\partial -\frac{1}{n}\sum_o{ln(Softmax_{oy(o)})}}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}\frac{\partial ln(Softmax_{iy(i)})}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}\frac{\partial (Score_{iy(i)}-ln(\sum_o{e^{Score_{io}}}))}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}(\frac{\partial Score_{iy(i)}}{\partial Score_{il}}-Softmax_{il})X_{ik}\\
&=\frac{1}{n}\sum_{i}(Softmax_{il}-C_{il})X_{ik},C_{il}=
\begin{cases}
1&l=y(i)\\
0&l\ne y(i)
\end{cases}\\
\end{align}
$$

因此可以得到外循环关于$$i$$，内循环关于$$l$$，循环体内根据上式计算部分梯度，最后将所有部分结果求和

### 2 整体求解

$$
\begin{align}
&\frac{\partial loss}{\partial W_{kl}}=\frac{1}{n}\sum_{i}(Softmax_{il}-C_{il})X_{ik}\\
\Rightarrow&\frac{\partial loss}{\partial W}=\frac{1}{n}\langle X^T,Softmax-C\rangle\\
\end{align}
$$

### 3 预测

同SVM的预测

## Neural Network

两层全连接神经网络，输入$$X_{N\times D}$$,标签$$y_{N\times 1}$$，隐藏层$$Layer1_{N\times H}$$，输出层$$Scores_{N\times C}$$

隐藏层需要加入激活函数，使用$$ReLU$$

### 1 输出层

$$Scores=\langle ReLU(\langle X,W1\rangle+b1),W2\rangle+b2$$

### 2 损失函数

$$
\begin{cases}
loss&=\frac{1}{N}\sum_i{-ln(Softmax_{iy(i)})}\\
Softmax_{ij}&=\frac{e^{Score_{ij}}}{\sum_k{e^{Score_{ik}}}}
\end{cases}
$$

### 3 梯度

#### 3.1 W2

$$
\begin{align}
\frac{\partial loss}{\partial W2_{kl}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial W2_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial W2_{kl}}}\\
&=\frac{1}{N}\sum_{i}{(Softmax_{ij}-C_{ij})ReLU_{ik}}\\
\frac{\partial loss}{\partial W2}&=\frac{1}{N}\langle ReLU^T,Softmax-C\rangle,C_{il}=
\begin{cases}
1&l=y(i)\\
0&l\ne y(i)
\end{cases}\\
\end{align}
$$

#### 3.2 b2

$$
\begin{align}
\frac{\partial loss}{\partial b2_{l}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial b2_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial b2_{l}}}\\
&=\frac{1}{N}\sum_{i}{(Softmax_{il}-C_{il})}
\end{align}
$$

#### 3.3 W1

$$
\begin{align}
\frac{\partial loss}{\partial W1_{kl}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{i^1j^1}\frac{\partial Scores_{ij}}{\partial ReLU_{i^1j^1}} \frac{\partial ReLU_{i^1j^1}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}\frac{\partial ReLU_{ij^1}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}D_{ij^1}\frac{\partial Layer1_{ij^1}}{\partial W1_{kl}}},D_{ij^1}=
\begin{cases}
1&ReLU_{ij^1}\gt 0\\
0&ReLU_{ij^1}\leq 0
\end{cases}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})W2_{lj}D_{il}X_{ik}}\\
\frac{\partial loss}{\partial W1}&=\langle X^T,\langle Softmax-C,W2^T\rangle*D\rangle
\end{align}
$$

#### 3.2 b1

$$
\begin{align}
\frac{\partial loss}{\partial b1_{l}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}D_{ij^1}\frac{\partial Layer1_{ij^1}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})W2_{lj}D_{il}}\\
&=\frac{1}{N}\sum_{i}(\langle Softmax-C,W2^T\rangle*D)_{il}
\end{align}
$$

### 4 预测

$$Scores=\langle ReLU(\langle X,W1\rangle+b1),W2\rangle+b2$$

$${y\_pred_i}=argmax(Scores_i)$$

### 5 自行训练

调参，略
