---
layout: page
title: CS231N 解题报告
date: 2018-07-06 15:20:19 +0800
mdate: 2018-10-24 15:27:00 +0800
showbar: false
---

*本文不包含任何代码！*

## Assignment 1

### 1 KNN

训练数据$$Train_{m\times p}$$，测试数据$$Test_{n\times p}$$，测试数据和训练数据之间的距离矩阵$$dist_{n\times m}$$

距离矩阵计算公式：$$ dist_{ij}=\sqrt{\sum_{k\in p}{(Test_{ik}-Train_{jk})^2}} $$

#### 1.1 双重循环

$$dist_{ij}$$的两个脚标分别循环

#### 1.2 单重循环

只对$$i$$循环，即以行为单位，利用的是*numpy*的广播$$dist_{i}=\sqrt{\sum_{k\in p}{(Test_{ik}-Train_{\#k})^2}}$$

> $$\#k$$代表矩阵第$$k$$列的列向量

> 单重循环实际上比双重循环慢

#### 1.3 无循环

不进行循环，同样利用*numpy*的广播，把平方项展开：

$$
\begin{align}
dist_{ij}&=\sqrt{\sum_{k}{(Test_{ik}-Train_{jk})^2}}\\
&=\sqrt{\sum_{k}{Test_{ik}^2}+\sum_{k}{Train_{jk}^2}-2\sum_{k}{Test_{ik}Train_{jk}}}
\end{align}
$$

根号中：第一项与$$j$$无关，对$$Test$$每行求平方和后的列向量广播到$$dist$$的每一列；第二项与$$i$$无关，对$$Train^T$$的每列求平方和后得到行向量广播到$$dist$$的每一行；第三项正好是矩阵乘法。所以有：

$$
\begin{align}
dist&=\sqrt{-2\langle Test,Train^T\rangle+Te_{n\times 1}+Tr_{1\times m}}
\end{align}
$$

其中$$Te_i=\sum_{k}{Test_{ik}^2}$$，$$Tr_j=\sum_{k}{Train_{jk}^2}$$

#### 1.4 Cross-Validation

训练集$$Train$$均分成5个Fold。每个batch取4个Fold训练，剩下的一个验证，共5种组合5个batch。每个batch需要验证所有待选的$$k$$。

### 2 SVM

数据预处理：所有数据集做相同的常数偏移，常数取使得训练数据0均值的值

对训练数据扩展一维$$TrainEx_{n\times (p+1)}$$，$$TrainEx_{i}=\begin{bmatrix}Train_{i}&1\end{bmatrix}$$，齐次空间

$$
\begin{cases}
loss&=reg\cdot {\Vert W \Vert}^2+\frac{1}{n}\sum_i^n\sum_j^mmargin_{ij}\\
grad&=2reg\cdot W+\frac{1}{n}\sum_i^n\sum_j^m{\nabla margin_{ij}}\\
margin_{ij}&=\begin{cases}max(0,\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1)&j\ne y_i\\0&j=y_i\\\end{cases}
\end{cases}
$$

> 目标是数据label在标记的那一维上的score超过其它维1以上，否则loss按不足部分线性增加

#### 2.1 循环计算

由于整体难以计算，因此单独计算$$margin_{ij}$$及其梯度，然后循环求和。只有非0项才参与$$loss$$和$$grad$$的计算，因此有

$$
\begin{align}
margin_{ij}&=\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1\\
\nabla margin_{ij}&=\nabla \langle X,W\rangle_{ij}-\nabla \langle X,W\rangle_{iy(i)}\\
&=\begin{bmatrix}
0_{00}&\cdots&0_{0(j-1)}&X_{i0}&0_{0(j+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(j-1)}&X_{i1}&0_{1(j+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(j-1)}&X_{i(n-1)}&0_{(n-1)(j+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}\\
&-
\begin{bmatrix}
0_{00}&\cdots&0_{0(y_i-1)}&X_{y_i0}&0_{0(y_i+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(y_i-1)}&X_{y_i1}&0_{1(y_i+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(y_i-1)}&X_{y_i(n-1)}&0_{(n-1)(y_i+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}
\end{align}
$$

#### 2.2 整体计算

因为只有$$margin_{ij}\neq 0$$的情况下才有必要计算，因此可以构造辅助矩阵来记录其是否为0

$$
\begin{align}
dW_{ij}&=\sum_k^n\sum_l^m({\nabla \langle X,W\rangle_{kl}}_ {ij} -{\nabla \langle X,W\rangle_{ky(k)}}_{ij})\\
&=\sum_{k,margin_{kj}\gt 0}^n{X_{ki}}-\sum_{k,y(k)=j}^n\sum_{l,margin_{kl}\gt 0}^p{X_{ki}}
\end{align}
$$

$$
\begin{align}
dW&=\langle X^T,M1\rangle-\langle X^T,M2\rangle\\
M1_{ij}&=
\begin{cases}
1&margin_{ij}>0\\
0&otherwise
\end{cases}\\
M2_{ij}&=
\begin{cases}
\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
\end{align}
$$

记辅助矩阵$$C_{n\times m}=M1-M2$$，有：

$$
C_{ij}=
\begin{cases}
1&margin_{ij}>0\\
-\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
$$

化简后的梯度计算：

$$\sum_i^n\sum_j^m{\nabla margin_{ij}}=\langle X^T,C\rangle$$

#### 2.3 SGD

随机选择*batch*，计算$$loss$$和$$grad$$

梯度下降$$W=W-lr\cdot grad$$

#### 2.4 预测

遍历所有*lr*和*L2*系数的组合，获得验证精度最高的*svm*

### 3 Softmax

输入$$X_{n\times d}$$，参数$$W_{d\times c}$$，标签$$y_{n\times 1}$$

首先是线性层：

$$Score=\langle X,W\rangle$$

$$Softmax$$值：

$$Softmax_{ij}=\frac{e^{Score_{ij}}}{\sum_k{e^{Score_{ik}}}}$$

损失函数和梯度：

$$
\begin{cases}
loss&=L2-\frac{1}{n}\sum_i{ln(Softmax_{iy(i)})}\\
grad&=\nabla L2+\frac{1}{n}\sum_i\frac{1}{Softmax_{iy(i)}}\nabla Softmax_{iy(i)}
\end{cases}
$$

#### 3.1 循环求解

主要关注$$Softmax$$矩阵，主循环对该矩阵的每一行进行计算

损失直接代公式即可，对于梯度

$$
\begin{align}
\frac{\partial loss}{\partial W_{kl}}&=\sum_{i,j}\frac{\partial loss}{\partial Score_{ij}}\frac{\partial Score_{ij}}{\partial W_{kl}}\\
&=\sum_{i}\frac{\partial loss}{\partial Score_{il}}X_{ik}\\
&=\sum_{i}\frac{\partial -\frac{1}{n}\sum_o{ln(Softmax_{oy(o)})}}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}\frac{\partial ln(Softmax_{iy(i)})}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}\frac{\partial (Score_{iy(i)}-ln(\sum_o{e^{Score_{io}}}))}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}(\frac{\partial Score_{iy(i)}}{\partial Score_{il}}-Softmax_{il})X_{ik}\\
&=\frac{1}{n}\sum_{i}(Softmax_{il}-C_{il})X_{ik},C_{il}=
\begin{cases}
1&l=y(i)\\
0&l\ne y(i)
\end{cases}\\
\end{align}
$$

> 这里的$$loss$$不包含$$L2$$项

因此可以得到外循环关于$$i$$，内循环关于$$l$$，循环体内根据上式计算部分梯度，最后将所有部分结果求和

#### 3.2 整体求解

$$
\begin{align}
&\frac{\partial loss}{\partial W_{kl}}=\frac{1}{n}\sum_{i}(Softmax_{il}-C_{il})X_{ik}\\
\Rightarrow&\frac{\partial loss}{\partial W}=\frac{1}{n}\langle X^T,Softmax-C\rangle\\
\end{align}
$$

#### 3.3 预测

同2.4

### 4 Neural Network

两层全连接神经网络，输入$$X_{N\times D}$$,标签$$y_{N\times 1}$$，隐藏层$$Layer1_{N\times H}$$，输出层$$Scores_{N\times C}$$

隐藏层需要加入激活函数，使用$$ReLU$$

$$
\begin{cases}
Score&=\langle ReLU(\langle X,W1\rangle+b1),W2\rangle+b2\\
Softmax_{ij}&=\frac{e^{Score_{ij}}}{\sum_k{e^{Score_{ik}}}}\\
loss&=\frac{1}{N}\sum_i{-ln(Softmax_{iy(i)})}\\
grad&=\frac{1}{N}\sum_i\frac{1}{Softmax_{iy(i)}}\nabla Softmax_{iy(i)}
\end{cases}
$$

#### 4.1 正向

$$Softmax$$可以用*numpy*的广播整体计算，因而$$loss$$可以直接用上述定义直接计算

#### 4.2 反向

#### 4.2.1 W2

$$
\begin{align}
\frac{\partial loss}{\partial W2_{kl}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial W2_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial W2_{kl}}}\\
&=\frac{1}{N}\sum_{i}{(Softmax_{ij}-C_{ij})ReLU_{ik}}\\
\frac{\partial loss}{\partial W2}&=\frac{1}{N}\langle ReLU^T,Softmax-C\rangle,C_{il}=
\begin{cases}
1&l=y(i)\\
0&l\ne y(i)
\end{cases}\\
\end{align}
$$

#### 4.2.2 b2

$$
\begin{align}
\frac{\partial loss}{\partial b2_{l}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial b2_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial b2_{l}}}\\
&=\frac{1}{N}\sum_{i}{(Softmax_{il}-C_{il})}
\end{align}
$$

#### 4.2.3 W1

$$
\begin{align}
\frac{\partial loss}{\partial W1_{kl}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{i^1j^1}\frac{\partial Scores_{ij}}{\partial ReLU_{i^1j^1}} \frac{\partial ReLU_{i^1j^1}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}\frac{\partial ReLU_{ij^1}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}D_{ij^1}\frac{\partial Layer1_{ij^1}}{\partial W1_{kl}}},D_{ij^1}=
\begin{cases}
1&ReLU_{ij^1}\gt 0\\
0&ReLU_{ij^1}\leq 0
\end{cases}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})W2_{lj}D_{il}X_{ik}}\\
\frac{\partial loss}{\partial W1}&=\langle X^T,\langle Softmax-C,W2^T\rangle\circ D\rangle
\end{align}
$$

#### 4.2.4 b1

$$
\begin{align}
\frac{\partial loss}{\partial b1_{l}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}D_{ij^1}\frac{\partial Layer1_{ij^1}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})W2_{lj}D_{il}}\\
&=\frac{1}{N}\sum_{i}(\langle Softmax-C,W2^T\rangle\circ D)_{il}
\end{align}
$$

#### 4.3 预测

$$Scores=\langle ReLU(\langle X,W1\rangle+b1),W2\rangle+b2$$

$${y\_pred_i}=argmax(Scores_i)$$

#### 4.4 自行训练

调参，略

### 5 特征

调参，略，只给结果

#### 5.1 SVM

$$
lr=10^{-8}\\
reg=5\cdot 10^6
$$

测试集43.2%

#### 5.2 NN

$$
hidden=500\\
num\_iters=9000\\
batch\_size=200\\
lr=0.1\\
reg=0.0003
$$

测试集56%

## Assignment 2

### 1 FCN

#### 1.1 仿射正向传播

$$y=\langle x,w\rangle+b$$

#### 1.2 仿射反向传播

计上层结果对$$y$$的偏导数为$$dy$$

$$
\begin{align}
dx_{ik}&=\sum_{j}{dy_{ij}\frac{\partial y_{ij}}{\partial x_{ik}}}\\
&=\sum_{j}dy_{ij}w_{kj}\\
dw_{kj}&=\sum_{i}{dy_{ij}\frac{\partial y_{ij}}{\partial w_{kj}}}\\
&=\sum_{i}dy_{ij}x_{ik}\\
db_j&=\sum_idy_{ij}
\end{align}
$$

整合后得

$$
\begin{align}
dx&=\langle dy,w^T\rangle\\
dw&=\langle x^T,dy\rangle\\
db&=np.sum(dy, axis=0)
\end{align}
$$

#### 1.3 ReLU正向传播

$$y_{ij}=max(0,x_{ij})$$

#### 1.4 ReLU反向传播

$$dx_{ij}=\
\begin{cases}
dy_{ij}&x_{ij}\gt 0\\
0&x_{ij}\leq 0\\
\end{cases}
$$

#### 1.5 重写二层全连接网络

把Assign1里的隐藏层封成一个函数，略（这里直接提示L2要带0.5）

#### 1.6 自己训练一个Solver

略

#### 1.7 多层全连接网络

同1.5，多几层隐藏层，略（这里直接提示L2要带0.5）

#### 1.8 & 1.9 训练至过拟合

3层

$$
ws=0.01\\
lr=0.01
$$

5层

$$
ws=0.1\\
lr=0.002
$$

5层网络在初始值过小的时候难以收敛，在初始值较大的时候收敛更快

#### 1.10 SGD+Momentum

梯度下降时带上“惯性”，初始动量为0，每次迭代增加的动量在下次迭代时部分保留

$$
\begin{align}
v&=mv-lr\cdot dw\\
w&=w+v
\end{align}
$$

这里用$$v$$代替*SGD*中的$$-lr\cdot dw$$。其中$$m$$是一个衰减系数，表示上一次迭代的增量在本次迭代中保留的比例。

#### 1.11 RMSProp和Adam

RMSProp

$$
\begin{align}
cache&=decay*cache+(1-decay)\cdot dw^2\\
w&=w-\frac{lr\cdot dw}{\sqrt{cache}+\epsilon}
\end{align}
$$

Adam

$$
\begin{align}
m&=\beta_1\cdot m+(1-\beta_1)\cdot dw\\
m_t&=\frac{m}{1-\beta_1^t}\\
v&=\beta_2\cdot v+(1-\beta_2)\cdot dw^2\\
v_t&=\frac{v}{1-\beta_2^t}\\
w&=w-\frac{lr\cdot m_t}{\sqrt{v_t}+\epsilon}
\end{align}
$$

#### 1.12 训练一个模型

可调超参：隐藏层数量，参数初始缩放，L2系数，批大小，优化器（SGD，SGD+Momentum，RMSProp，Adam），学习率，学习率衰减。略，最终测试集上应该可以到55%以上

### 2 Batch Normalization

训练时将每个*batch*缩放到$$0$$均值、$$1$$方差，新增超参$$\gamma$$作为“新标准差”，$$\beta$$作为“新均值”，用新超参再将*batch*回放到原空间

测试时不能用*batch*的均值和标准差，所以需要训练时同时记录额外信息：

$$
\begin{align}
mean&=p\cdot mean+(1-p)\cdot batch\_mean\\
variance&=p\cdot variance+(1-p)\cdot batch\_variance
\end{align}
$$

测试时用$$mean$$和$$variance$$分别代替训练时的$$batch\_mean$$和$$batch\_variance$$

#### 2.1 Forward Pass

$$
\begin{align}
\hat{X}_i&=\frac{X_i-\frac{1}{n}\sum_jX_j}{\sqrt{\frac{1}{n}\sum_j(X_j-\frac{1}{n}\sum_kX_k)^2+\epsilon}}\\
y_i&=\hat{X}_i*\gamma+\beta
\end{align}
$$

#### 2.2 Backward Pass

$$
\begin{align}
dX_{ij}&=\sum_k{dy_{kj}\cdot \frac{\partial y_{kj}}{\partial X_{ij}}}\\
&=\gamma\sum_k{dy_{kj}\cdot \frac{\partial \hat{X}_{kj}}{\partial X_{ij}}}=\gamma\sum_k{dy_{kj}\cdot \frac{\partial \frac{X_{kj}-u_j}{\sigma_j}}{\partial X_{ij}}}\\
&=\gamma\sum_k{dy_{kj}\frac{\sigma_j \frac{\partial(X_{kj}-u_j)}{\partial X_{kj}}-(X_{kj}-u_j)\frac{\partial \sigma_j}{\partial X_{kj}}}{\sigma_j^2}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{\sigma_j^2}\sum_k{dy_{kj}(X_{kj}-u_j)\frac{\partial \sigma_j}{\partial X_{ij}}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{2n\sigma_j^3}\sum_k{dy_{kj}(X_{kj}-u_j)\sum_l{\frac{\partial (X_{lj}-u_j)^2}{\partial X_{ij}}}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{n\sigma_j^3}\sum_k{dy_{kj}(X_{kj}-u_j)\sum_l{(X_{lj}-u_j)\frac{\partial (X_{lj}-u_j)}{\partial X_{ij}}}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{n\sigma_j^3}\sum_k{dy_{kj}(X_{kj}-u_j)(X_{ij}-u_j)}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{(X_{ij}-u_j)}{n}\sum_k{\frac{\gamma dy_{ij}}{\sigma_j^3}(X_{kj}-u_j)}\\
d\gamma_j&=\sum_k{dy_{kj}\hat{X}_{kj}}\\
d\beta_j&=\sum_k{dy_{kj}}
\end{align}
$$

#### 2.3 给全连接网络增加BN层

注意BN加的位置是在全连接层之后非线性层之前，所以是Affine->BatchNorm->ReLU

#### 2.4 其它

直接跑就行了

#### 2.5 LayerNorm

TODO

### 3 Dropout

TODO

### 4 CNN

TODO

### 5 Pytorch or Tensorflow
