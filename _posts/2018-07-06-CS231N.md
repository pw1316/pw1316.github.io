---
layout: page
title: CS231N 解题报告
showbar: false
---

*本文不包含任何代码！*

*<>为矩阵点积，\*为矩阵逐元素相乘*

# Assignment 1

## KNN

训练数据$$Train_{m\times p}$$，测试数据$$Test_{n\times p}$$，测试数据和训练数据之间的距离矩阵$$distance_{n\times m}$$

距离矩阵计算公式：$$ distance_{ij}=\sqrt{\sum_{k}^{p}{(Test_{ik}-Train_{jk})^2}} $$

### 1 双循环

循环体内单独计算$$distance_{ij}$$，直接按照公式计算即可

### 2 单循环

只对i循环，整行一起计算，利用numpy的广播$$ distance_{i}=\sqrt{\sum_{k}^{p}{(Test_{ik}-Train_{column\ k})^2}} $$

单循环比双循环慢

### 3 无循环

整个矩阵一起计算，同样利用numpy的广播，把平方项展开：

$$
\begin{align}
distance_{ij}&=\sqrt{\sum_{k}{(Test_{ik}-Train_{jk})^2}}\\
&=\sqrt{\sum_{k}{Test_{ik}^2}+\sum_{k}{Train_{jk}^2}-2\sum_{k}{Test_{ik}*Train_{jk}}}
\end{align}
$$

根号中：第一项与$$j$$无关，对$$Test$$每行求平方和后的列向量广播到$$distance$$的每一列；第二项与$$i$$无关，对$$Train$$的每行求平方和后转置得到行向量广播到$$distance$$的每一行；第三项正好是矩阵乘法。所以有：

$$
\begin{align}
distance&=\sqrt{-2\langle Test,Train^T\rangle+Te_{n\times 1}+Tr_{1\times m}}
\end{align}
$$

其中$$Te_i=\sum_{k}{Test_{ik}^2}$$，$$Tr_j=\sum_{k}{Train_{jk}^2}$$

### 4 Cross-Validation

5个Fold，5个batch，每个batch取4个Fold训练，剩下的一个验证。每个batch需要验证所有的k值。

外循环batch，内循环k，计算每个batch不同k的精度

从图上取最佳的k作为最终值

## SVM

数据预处理：使得训练数据0均值，其余数据集做相同加减

### 线性SVM

对训练数据扩展一维$$TrainEx_{m\times (p+1)}$$，$$TrainEx_{i}=\begin{bmatrix}Train_{i}&1\end{bmatrix}$$，消掉偏移项

$$
\begin{cases}
loss&=reg*{\Vert W \Vert}^2+\frac{1}{n}\sum_i^n\sum_j^mmargin_{ij}\\
grad&=2reg*W+\frac{1}{n}\sum_i^n\sum_j^m{\nabla margin_{ij}}\\
margin_{ij}&=\begin{cases}max(0,\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1)&j\ne y_i\\0&j=y_i\\\end{cases}
\end{cases}
$$

### 1 循环计算

由于整体难以计算，因此单独计算$$margin_{ij}$$及其梯度，然后循环求和。只有非0项才参与$$loss$$和$$grad$$的计算，因此有

$$
\begin{align}
margin_{ij}&=\langle X,W\rangle_{ij}-\langle X,W\rangle_{iy_i}+1\\
\nabla margin_{ij}&=\nabla \langle X,W\rangle_{ij}-\nabla \langle X,W\rangle_{iy(i)}\\
&=\begin{bmatrix}
0_{00}&\cdots&0_{0(j-1)}&X_{i0}&0_{0(j+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(j-1)}&X_{i1}&0_{1(j+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(j-1)}&X_{i(n-1)}&0_{(n-1)(j+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}\\
&-
\begin{bmatrix}
0_{00}&\cdots&0_{0(y_i-1)}&X_{y_i0}&0_{0(y_i+1)}&\cdots&0_{0(m-1)}\\
0_{10}&\cdots&0_{1(y_i-1)}&X_{y_i1}&0_{1(y_i+1)}&\cdots&0_{1(m-1)}\\
\vdots&\ddots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0_{(n-1)0}&\cdots&0_{(n-1)(y_i-1)}&X_{y_i(n-1)}&0_{(n-1)(y_i+1)}&\cdots&0_{(n-1)(m-1)}
\end{bmatrix}
\end{align}
$$

### 2 整体计算

利用numpy矩阵计算的优势

因为只有$$margin_{ij}\neq 0$$的情况下才有必要计算，因此可以构造辅助矩阵来记录其是否为0

$$
\begin{align}
dW_{ij}&=\sum_k^n\sum_l^m({\nabla \langle X,W\rangle_{kl}}_ {ij} -{\nabla \langle X,W\rangle_{ky(k)}}_{ij})\\
&=\sum_{k,margin_{kj}\gt 0}^n{X_{ki}}-\sum_{k,y(k)=j}^n\sum_{l,margin_{kl}\gt 0}^p{X_{ki}}
\end{align}
$$

$$
\begin{align}
dW&=\langle X^T,M1\rangle-\langle X^T,M2\rangle\\
M1_{ij}&=
\begin{cases}
1&margin_{ij}>0\\
0&otherwise
\end{cases}\\
M2_{ij}&=
\begin{cases}
\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
\end{align}
$$

记辅助矩阵$$C_{n\times m}=M1-M2$$，有：

$$
C_{ij}=
\begin{cases}
1&margin_{ij}>0\\
-\sum_{k,margin_{ik}\gt 0}^m{1}&j=y(i)\\
0&otherwise
\end{cases}
$$

化简后的梯度计算：

$$\sum_i^n\sum_j^m{\nabla margin_{ij}}=\langle X^T,C\rangle$$

### 3 SGD

随机选择batch，计算loss和grad

梯度下降$$W=W-lr*grad$$

### 4 预测

自定义lr和L2的range，两层循环遍历所有组合，记录验证精度最高的svm

## Softmax

输入$$X_{n\times d}$$，参数$$W_{d\times c}$$，标签$$y_{n\times 1}$$

首先是线性层：

$$Score=\langle X,W\rangle$$

Softmax值：

$$Softmax_{ij}=\frac{e^{Score_{ij}}}{\sum_k{e^{Score_{ik}}}}$$

损失函数和梯度：

$$
\begin{cases}
loss&=L2-\frac{1}{n}\sum_i{ln(Softmax_{iy(i)})}\\
grad&=\nabla L2+\frac{1}{n}\sum_i\frac{1}{Softmax_{iy(i)}}\nabla Softmax_{iy(i)}
\end{cases}
$$

### 1 循环求解

主要关注$$Softmax$$矩阵，主循环对该矩阵的每一行进行计算

损失直接代公式即可，对于梯度

$$
\begin{align}
\frac{\partial loss}{\partial W_{kl}}&=\sum_{i,j}\frac{\partial loss}{\partial Score_{ij}}\frac{\partial Score_{ij}}{\partial W_{kl}}\\
&=\sum_{i}\frac{\partial loss}{\partial Score_{il}}X_{ik}\\
&=\sum_{i}\frac{\partial -\frac{1}{n}\sum_o{ln(Softmax_{oy(o)})}}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}\frac{\partial ln(Softmax_{iy(i)})}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}\frac{\partial (Score_{iy(i)}-ln(\sum_o{e^{Score_{io}}}))}{\partial Score_{il}}X_{ik}\\
&=-\frac{1}{n}\sum_{i}(\frac{\partial Score_{iy(i)}}{\partial Score_{il}}-Softmax_{il})X_{ik}\\
&=\frac{1}{n}\sum_{i}(Softmax_{il}-C_{il})X_{ik},C_{il}=
\begin{cases}
1&l=y(i)\\
0&l\ne y(i)
\end{cases}\\
\end{align}
$$

因此可以得到外循环关于$$i$$，内循环关于$$l$$，循环体内根据上式计算部分梯度，最后将所有部分结果求和

### 2 整体求解

$$
\begin{align}
&\frac{\partial loss}{\partial W_{kl}}=\frac{1}{n}\sum_{i}(Softmax_{il}-C_{il})X_{ik}\\
\Rightarrow&\frac{\partial loss}{\partial W}=\frac{1}{n}\langle X^T,Softmax-C\rangle\\
\end{align}
$$

### 3 预测

同SVM的预测

## Neural Network

两层全连接神经网络，输入$$X_{N\times D}$$,标签$$y_{N\times 1}$$，隐藏层$$Layer1_{N\times H}$$，输出层$$Scores_{N\times C}$$

隐藏层需要加入激活函数，使用$$ReLU$$

### 1 输出层

$$Scores=\langle ReLU(\langle X,W1\rangle+b1),W2\rangle+b2$$

### 2 损失函数

$$
\begin{cases}
loss&=\frac{1}{N}\sum_i{-ln(Softmax_{iy(i)})}\\
Softmax_{ij}&=\frac{e^{Score_{ij}}}{\sum_k{e^{Score_{ik}}}}
\end{cases}
$$

### 3 梯度

#### 3.1 W2

$$
\begin{align}
\frac{\partial loss}{\partial W2_{kl}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial W2_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial W2_{kl}}}\\
&=\frac{1}{N}\sum_{i}{(Softmax_{ij}-C_{ij})ReLU_{ik}}\\
\frac{\partial loss}{\partial W2}&=\frac{1}{N}\langle ReLU^T,Softmax-C\rangle,C_{il}=
\begin{cases}
1&l=y(i)\\
0&l\ne y(i)
\end{cases}\\
\end{align}
$$

#### 3.2 b2

$$
\begin{align}
\frac{\partial loss}{\partial b2_{l}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial b2_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial b2_{l}}}\\
&=\frac{1}{N}\sum_{i}{(Softmax_{il}-C_{il})}
\end{align}
$$

#### 3.3 W1

$$
\begin{align}
\frac{\partial loss}{\partial W1_{kl}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{i^1j^1}\frac{\partial Scores_{ij}}{\partial ReLU_{i^1j^1}} \frac{\partial ReLU_{i^1j^1}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}\frac{\partial ReLU_{ij^1}}{\partial W1_{kl}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}D_{ij^1}\frac{\partial Layer1_{ij^1}}{\partial W1_{kl}}},D_{ij^1}=
\begin{cases}
1&ReLU_{ij^1}\gt 0\\
0&ReLU_{ij^1}\leq 0
\end{cases}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})W2_{lj}D_{il}X_{ik}}\\
\frac{\partial loss}{\partial W1}&=\langle X^T,\langle Softmax-C,W2^T\rangle*D\rangle
\end{align}
$$

#### 3.2 b1

$$
\begin{align}
\frac{\partial loss}{\partial b1_{l}}&=\frac{1}{N}\sum_{i,j}{\frac{\partial loss}{\partial Scores_{ij}} \frac{\partial Scores_{ij}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\frac{\partial Scores_{ij}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})\sum_{j^1}W2_{j^1j}D_{ij^1}\frac{\partial Layer1_{ij^1}}{\partial b1_{l}}}\\
&=\frac{1}{N}\sum_{i,j}{(Softmax_{ij}-C_{ij})W2_{lj}D_{il}}\\
&=\frac{1}{N}\sum_{i}(\langle Softmax-C,W2^T\rangle*D)_{il}
\end{align}
$$

### 4 预测

$$Scores=\langle ReLU(\langle X,W1\rangle+b1),W2\rangle+b2$$

$${y\_pred_i}=argmax(Scores_i)$$

### 5 自行训练

调参，略

## 特征

自己调参，下面给的是我的

### 1 SVM

$$
lr=10^{-8}\\
reg=5*10^6
$$

测试集43.2%

### 2 NN

$$
hidden=500\\
num\_iters=9000\\
batch\_size=200\\
lr=0.1\\
reg=0.0003
$$

测试集56%

# Assignment 2

## FCN

### 1 仿射正向传播

$$y=\langle x,w\rangle+b$$

### 2 仿射反向传播

计上层结果对$$y$$的偏导数为$$dy_{(N\times M)}$$

$$
\begin{align}
dx_{ik}&=\sum_{j}{\frac{\partial upper}{\partial y_{ij}} \frac{\partial y_{ij}}{\partial x_{ik}}}\\
&=\sum_{j}dy_{ij}w_{kj}\\
dx&=\langle dy,w^T\rangle\\
dw_{kj}&=\sum_{i}{\frac{\partial upper}{\partial y_{ij}} \frac{\partial y_{ij}}{\partial w_{kj}}}\\
&=\sum_{i}dy_{ij}x_{ik}\\
dw&=\langle x^T,dy\rangle\\
db_j&=\sum_idy_{ij}
\end{align}
$$

### 3 ReLU正向传播

$$y_{ij}=max(0,x_{ij})$$

### 4 ReLU反向传播

$$dx_{ij}=\
\begin{cases}
dy_{ij}&x_{ij}\gt 0\\
0&x_{ij}\leq 0\\
\end{cases}
$$

### 5 重写二层全连接网络

参数初始化，正反传播，略（这里直接提示L2要带0.5）

### 6 自己训练一个Solver

略

### 7 多层全连接网络

每层参数初始化，每层正反传播，略（这里直接提示L2要带0.5）

### 8 & 9 训练至过拟合

3层

$$
ws=0.01\\
lr=0.01
$$

5层

$$
ws=0.1\\
lr=0.002
$$

5层网络在初始值过小的时候难以收敛，在初始值较大的时候收敛更快

### 10 SGD+Momentum

梯度下降时带上“惯性”，初始动量为0，每次迭代增加的动量在下次迭代时部分保留

$$
\begin{align}
v&=mv-lr*dw\\
w&=w+v
\end{align}
$$

其中$$m$$是一个衰减系数，表示初始动量在下一次迭代时的衰减百分比，$$-lr*dw$$为新增的动量

### 11 RMSProp和Adam

RMSProp

$$
\begin{align}
cache&=decay*cache+(1-decay) *dw^2\\
w&=w-\frac{lr*dw}{\sqrt{cache}+\epsilon}
\end{align}
$$

Adam

$$
\begin{align}
m&=\beta_1*m+(1-\beta_1) *dw\\
m_t&=\frac{m}{1-\beta_1^t}\\
v&=\beta_2*v+(1-\beta_2) *dw^2\\
v_t&=\frac{v}{1-\beta_2^t}\\
w&=w-\frac{lr*m_t}{\sqrt{v_t}+\epsilon}
\end{align}
$$

### 12 训练一个模型

可调超参：隐藏层数量，参数初始缩放，L2系数，批大小，优化器（SGD，SGD+Momentum，RMSProp，Adam），学习率，学习率衰减。略，最终测试集上应该可以到55%以上

## Batch Normalization

BN层先把每个batch的数据偏移，缩放为0均值1方差，然后再统一缩放偏移回原空间

### 1 Forward Pass

$$
\begin{align}
\hat{X_i}&=\frac{X_i-\frac{1}{n}\sum_jX_j}{\sqrt{\frac{1}{n}\sum_j(X_j-\frac{1}{n}\sum_kX_k)^2+\epsilon}}\\
y_i&=\hat{X_i}*\gamma+\beta
\end{align}
$$

对于测试数据，没有batch的平均值和方差，因此要在运行时记录下来，采用momentum的方式每个batch更新

$$
\begin{align}
mean&=p*mean+(1-p) *batch\_mean\\
variance&=p*variance+(1-p) *batch\_variance
\end{align}
$$

### 2 Backward Pass

$$
\begin{align}
dX_{ij}&=\sum_k{dy_{kj}*\frac{\partial y_{kj}}{\partial X_{ij}}}\\
&=\gamma\sum_k{dy_{kj}*\frac{\partial \hat{X_{kj}}}{\partial X_{ij}}}=\gamma\sum_k{dy_{kj}*\frac{\partial \frac{X_{kj}-u_j}{\sigma_j}}{\partial X_{ij}}}\\
&=\gamma\sum_k{dy_{kj}\frac{\sigma_j \frac{\partial(X_{kj}-u_j)}{\partial X_{kj}}-(X_{kj}-u_j)\frac{\partial \sigma_j}{\partial X_{kj}}}{\sigma_j^2}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{\sigma_j^2}\sum_k{dy_{kj}(X_{kj}-u_j)\frac{\partial \sigma_j}{\partial X_{ij}}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{2n\sigma_j^3}\sum_k{dy_{kj}(X_{kj}-u_j)\sum_l{\frac{\partial (X_{lj}-u_j)^2}{\partial X_{ij}}}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{n\sigma_j^3}\sum_k{dy_{kj}(X_{kj}-u_j)\sum_l{(X_{lj}-u_j)\frac{\partial (X_{lj}-u_j)}{\partial X_{ij}}}}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{\gamma}{n\sigma_j^3}\sum_k{dy_{kj}(X_{kj}-u_j)(X_{ij}-u_j)}\\
&=\frac{\gamma dy_{ij}}{\sigma_j}-\frac{1}{n}\sum_k{\frac{\gamma dy_{kj}}{\sigma_j}}-\frac{(X_{ij}-u_j)}{n}\sum_k{\frac{\gamma dy_{ij}}{\sigma_j^3}(X_{kj}-u_j)}\\
d\gamma_j&=\sum_k{dy_{kj}\hat{X_{kj}}}\\
d\beta_j&=\sum_k{dy_{kj}}
\end{align}
$$

### 3 给全连接网络增加BN层

注意BN加的位置是在全连接层之后非线性层之前，所以是Affine->BatchNorm->ReLU。还是听课程建议自己写个帮助函数吧。

### 4 其它

BN剩下的部分就只要RUNRUNRUN就行了

### 5 LayerNorm Forward Pass

TODO

## Dropout

TODO

## CNN

TODO

## Pytorch or Tensorflow
